{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DSPy optimizer is an algorithm that can tune the parameters of a DSPy program (i.e., the prompts and/or the LM weights) to maximize the metrics you specify, like accuracy.\n",
    "\n",
    "There are many built-in optimizers in DSPy, which apply vastly different strategies. A typical DSPy optimizer takes three things:\n",
    "\n",
    "* Your DSPy program. This may be a single module (e.g., dspy.Predict) or a complex multi-module program.\n",
    "\n",
    "* Your metric. This is a function that evaluates the output of your program, and assigns it a score (higher is better).\n",
    "\n",
    "* A few training inputs. This may be very small (i.e., only 5 or 10 examples) and incomplete (only inputs to your program, without any labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does a DSPy Optimizer tune? How does it tune them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional deep neural networks (DNNs) can be optimized with gradient descent, given a loss function and some training data.\n",
    "\n",
    "DSPy programs consist of multiple calls to LMs, stacked togther as [DSPy modules]. Each DSPy module has internal parameters of three kinds: (1) the LM weights, (2) the instructions, and (3) demonstrations of the input/output behavior.\n",
    "\n",
    "Given a metric, DSPy can optimize all of these three with multi-stage optimization algorithms. These can combine gradient descent (for LM weights) and discrete LM-driven optimization, i.e. for crafting/updating instructions and for creating/validating demonstrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What DSPy Optimizers are currently available?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Few-Shot Learning[](https://dspy-docs.vercel.app/docs/building-blocks/optimizers#automatic-few-shot-learning)\n",
    "\n",
    "1. **`LabeledFewShot`**: Simply constructs few-shot examples from provided labeled Q/A pairs.\n",
    "2. **`BootstrapFewShot`**: Uses your program to self-generate complete demonstrations for every stage of your program. Will simply use the generated demonstrations (if they pass the metric) without any further optimization. Advanced: Supports using a teacher program (a different DSPy program that has compatible structure) and a teacher LM, for harder tasks.\n",
    "3. **`BootstrapFewShotWithRandomSearch`**: Applies `BootstrapFewShot` several times with random search over generated demonstrations, and selects the best program.\n",
    "4. **`BootstrapFewShotWithOptuna`**: Applies `BootstrapFewShot` through Optuna hyperparameter optimization across demonstration sets, running trials to maximize evaluation metrics.\n",
    "\n",
    "### Automatic Instruction Optimization[](https://dspy-docs.vercel.app/docs/building-blocks/optimizers#automatic-instruction-optimization)\n",
    "\n",
    "1. **`SignatureOptimizer`**: Generates and refines new instructions for each step, and optimizes them with coordinate ascent.\n",
    "2. **`BayesianSignatureOptimizer`**: Generates instructions and few-shot examples in each step. The instruction generation is data-aware and demonstration-aware. Uses Bayesian Optimization to effectively search over the space of generation instructions/demonstrations across your modules.\n",
    "\n",
    "### Automatic Finetuning[](https://dspy-docs.vercel.app/docs/building-blocks/optimizers#automatic-finetuning)\n",
    "\n",
    "1. **`BootstrapFinetune`**: Distills a prompt-based DSPy program into weight updates (for smaller LMs). The output is a DSPy program that has the same steps, but where each step is conducted by a finetuned model instead of a prompted LM.\n",
    "\n",
    "### Program Transformations[](https://dspy-docs.vercel.app/docs/building-blocks/optimizers#program-transformations)\n",
    "\n",
    "1. **`KNNFewShot`**. Selects demonstrations through k-Nearest Neighbors algorithm integrating `BootstrapFewShot` for bootstrapping/selection process.\n",
    "2. **`Ensemble`**: Ensembles a set of DSPy programs and either uses the full set or randomly samples a subset into a single program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which optimizer should I use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "- If you have very little data, e.g. 10 examples of your task, use `BootstrapFewShot`.\n",
    "- If you have slightly more data, e.g. 50 examples of your task, use `BootstrapFewShotWithRandomSearch`.\n",
    "- If you have more data than that, e.g. 300 examples or more, use `BayesianSignatureOptimizer`.\n",
    "- If you have been able to use one of these with a large LM (e.g., 7B parameters or above) and need a very efficient program, compile that down to a small LM with `BootstrapFinetune`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do I use an optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "# Set up the optimizer: we want to \"bootstrap\" (i.e., self-generate) 8-shot examples of your program's steps.\n",
    "# The optimizer will repeat this 10 times (plus some initial attempts) before selecting its best attempt on the devset.\n",
    "config = dict(max_bootstrapped_demos=3, max_labeled_demos=3, num_candidate_programs=10, num_threads=4)\n",
    "\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(metric=YOUR_METRIC_HERE, **config)\n",
    "optimized_program = teleprompter.compile(YOUR_PROGRAM_HERE, trainset=YOUR_TRAINSET_HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
